"""
GPU ìµœì í™”ëœ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ ê°ì§€ ëª¨ë“ˆ (ì™„ì „ ë¦¬íŒ©í† ë§ë¨)
- ëª¨ë“ˆëŸ¬ ëª¨ë¸ ì•„í‚¤í…ì²˜
- ë™ì  ëª¨ë¸ êµì²´ ì§€ì›
- GPU ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
"""

import torch
import numpy as np
import cv2
from pathlib import Path
import os
from typing import Optional, List, Tuple, Dict, Any
import logging
from tqdm import tqdm
import time
import pickle

# í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ image_utils import
try:
    from image_utils import get_target_roi_info
except ImportError:
    # image_utilsê°€ ì—†ì–´ë„ ë™ì‘í•˜ë„ë¡ fallback í•¨ìˆ˜ ì œê³µ
    def get_target_roi_info(image_path):
        """ROI ìë™ ê°ì§€ í•¨ìˆ˜ (fallback)"""
        return None, None, "image_utils not available"

# ìƒˆë¡œìš´ ëª¨ë“ˆëŸ¬ ëª¨ë¸ ì‹œìŠ¤í…œ import
import sys
from pathlib import Path

# ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent
models_dir = project_root / "models"
sys.path.insert(0, str(models_dir))

from model_factory import ModelFactory
from base_model import BaseAnomalyModel

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ [GPU Optimized] Using device: {DEVICE}")
if torch.cuda.is_available():
    print(f"ğŸ”¥ [GPU Info] GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ [GPU Info] GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")


class GPUDeepAnomalyDetector:
    """GPU ìµœì í™”ëœ ë”¥ëŸ¬ë‹ ì´ìƒ ê°ì§€ ê²€ì¶œê¸° (ëª¨ë“ˆëŸ¬ êµ¬ì¡°)"""
    
    def __init__(self, model_name: str = "patchcore", config: Optional[Dict] = None):
        """
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ ('patchcore', 'padim', 'fastflow' ë“±)
            config: ëª¨ë¸ë³„ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ê¸°ë³¸ ì„¤ì •
        self.model_name = model_name
        self.config = config or {}
        
        # í•´ìƒë„ ë¹„ìœ¨ ì„¤ì • (ê¸°ë³¸ê°’: 35%)
        self.resolution_ratio = self.config.get('resolution_ratio', 0.35)
        
        print(f"ğŸ¤– [GPU Detector] Using {model_name} model")
        print(f"ğŸ’» [Device] {self.device}")
        print(f"ğŸ“ [Resolution] Initial ratio: {self.resolution_ratio*100:.0f}%")
        
        # ëª¨ë¸ íŒ©í† ë¦¬ë¥¼ í†µí•œ ëª¨ë¸ ìƒì„±
        try:
            self.model = ModelFactory.create_model(
                model_name=model_name,
                device=self.device,
                config=self.config
            )
            print(f"âœ… [GPU Detector] Model created successfully")
        except ValueError as e:
            print(f"âŒ [GPU Detector] Model creation failed: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ PatchCore ì‚¬ìš©
            print(f"ğŸ”„ [GPU Detector] Falling back to PatchCore")
            self.model = ModelFactory.create_model(
                model_name="patchcore",
                device=self.device,
                config=self.config
            )
            self.model_name = "patchcore"
    
    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return ModelFactory.get_available_models()
    
    def switch_model(self, model_name: str, config: Optional[Dict] = None) -> bool:
        """
        ëª¨ë¸ êµì²´
        
        Args:
            model_name: ìƒˆ ëª¨ë¸ ì´ë¦„
            config: ìƒˆ ëª¨ë¸ ì„¤ì •
            
        Returns:
            êµì²´ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ê¸°ì¡´ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            if hasattr(self.model, 'clear_memory'):
                self.model.clear_memory()
            
            # ìƒˆ ëª¨ë¸ ìƒì„±
            new_config = self.config.copy()
            if config:
                new_config.update(config)
            
            self.model = ModelFactory.create_model(
                model_name=model_name,
                device=self.device,
                config=new_config
            )
            
            self.model_name = model_name
            self.config = new_config
            
            print(f"ğŸ”„ [GPU Detector] Model switched to {model_name}")
            return True
            
        except Exception as e:
            print(f"âŒ [GPU Detector] Model switch failed: {e}")
            return False
    
    def set_resolution_ratio(self, ratio: float):
        """í•´ìƒë„ ë¹„ìœ¨ ì„¤ì •"""
        if 0.2 <= ratio <= 1.0:
            self.resolution_ratio = ratio
            # ëª¨ë¸ì˜ í•´ìƒë„ ë¹„ìœ¨ë„ ì—…ë°ì´íŠ¸
            if hasattr(self.model, 'resolution_ratio'):
                self.model.resolution_ratio = ratio
            if hasattr(self.model, 'config') and 'resolution_ratio' in self.model.config:
                self.model.config['resolution_ratio'] = ratio
            print(f"ğŸ“ [Resolution] Updated ratio to {ratio*100:.0f}%")
        else:
            print(f"âŒ [Resolution] Invalid ratio {ratio}, must be between 0.2 and 1.0")
            
    def fit(self, normal_images: List[np.ndarray], progress_callback: Optional[callable] = None):
        """ëª¨ë¸ í›ˆë ¨"""
        print(f"ğŸ¯ [Training] Starting training with {len(normal_images)} normal images")
        
        # ì´ë¯¸ì§€ ë°°ì¹˜ë¡œ ë¶„í• 
        batch_size = self.config.get('batch_size', 8)
        image_batches = []
        
        for i in range(0, len(normal_images), batch_size):
            batch = normal_images[i:i + batch_size]
            image_batches.append(batch)
        
        print(f"ğŸ“¦ [Training] Created {len(image_batches)} batches (batch_size={batch_size})")
        
        # ëª¨ë¸ í›ˆë ¨
        start_time = time.time()
        result = self.model.train(image_batches, progress_callback)
        training_time = time.time() - start_time
        
        print(f"â±ï¸ [Training] Completed in {training_time:.2f} seconds")
        return result
        
    def predict(self, image_path: str) -> Tuple[bool, float]:
        """ì´ìƒ ê°ì§€ ì˜ˆì¸¡"""
        if isinstance(image_path, str):
            image = cv2.imread(image_path)
        else:
            image = image_path
            
        if image is None:
            return False, 0.0
        
        # ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ë³€í™˜
        images_batch = [[image]]
        
        try:
            scores, _ = self.model.predict(images_batch)
            score = scores[0] if len(scores) > 0 else 0.0
            
            # ì„ê³„ê°’ ê¸°ë°˜ ì´ìƒ íŒì • (ì„ì‹œë¡œ í‰ê·  + 2*í‘œì¤€í¸ì°¨ ì‚¬ìš©)
            # TODO: ë” ì •êµí•œ ì„ê³„ê°’ ì„¤ì • ë°©ë²• êµ¬í˜„
            is_anomaly = score > 0.5  # ì„ì‹œ ì„ê³„ê°’
            
            return is_anomaly, float(score)
            
        except Exception as e:
            print(f"âŒ [Prediction] Error: {e}")
            return False, 0.0
        
    def create_heatmap(self, image_path: str, patch_stride: int = 16) -> Optional[np.ndarray]:
        """GPU ìµœì í™”ëœ íˆíŠ¸ë§µ ìƒì„±"""
        if isinstance(image_path, str):
            image = cv2.imread(image_path)
        else:
            image = image_path
            
        if image is None:
            return None
            
        if not self.model.is_trained:
            raise ValueError("Model not fitted")
            
        print(f"ğŸ”¥ [GPU Heatmap] Creating heatmap for {image.shape} image")
        
        try:
            # ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ë³€í™˜
            images_batch = [[image]]
            
            # ì´ìƒ ì ìˆ˜ì™€ ë§µ ê³„ì‚°
            scores, anomaly_maps = self.model.predict(images_batch)
            
            if len(anomaly_maps) > 0:
                anomaly_map = anomaly_maps[0]
                
                # íˆíŠ¸ë§µì„ ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                h, w = image.shape[:2]
                heatmap = cv2.resize(anomaly_map, (w, h))
                
                # 0-255 ë²”ìœ„ë¡œ ì •ê·œí™”
                heatmap = ((heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8) * 255).astype(np.uint8)
                
                print(f"âœ… [GPU Heatmap] Heatmap created successfully")
                return heatmap
            else:
                print(f"âŒ [GPU Heatmap] No anomaly map generated")
                return None
                
        except Exception as e:
            print(f"âŒ [GPU Heatmap] Error: {e}")
            return None
    
    def save_model(self, save_dir: str) -> bool:
        """ëª¨ë¸ ì €ì¥"""
        try:
            save_path = self.model.save_model(Path(save_dir))
            print(f"ğŸ’¾ [Save] Model saved to: {save_path}")
            return True
        except Exception as e:
            print(f"âŒ [Save] Failed to save model: {e}")
            return False
    
    def load_model(self, model_path: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            # íŒŒì¼ì—ì„œ ëª¨ë¸ ë¡œë“œ
            self.model = ModelFactory.load_model_from_file(
                model_path=Path(model_path),
                device=self.device
            )
            
            # í•´ìƒë„ ë¹„ìœ¨ ë™ê¸°í™”
            if hasattr(self.model, 'resolution_ratio'):
                self.resolution_ratio = self.model.resolution_ratio
            
            print(f"ğŸ“‚ [Load] Model loaded from: {model_path}")
            return True
            
        except Exception as e:
            print(f"âŒ [Load] Failed to load model: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        base_info = {
            'detector_name': 'GPUDeepAnomalyDetector',
            'current_model': self.model_name,
            'device': str(self.device),
            'resolution_ratio': self.resolution_ratio,
            'available_models': self.get_available_models()
        }
        
        # ëª¨ë¸ë³„ ìƒì„¸ ì •ë³´ ì¶”ê°€
        if hasattr(self.model, 'get_model_info'):
            model_info = self.model.get_model_info()
            base_info.update(model_info)
        
        return base_info
    
    def clear_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if hasattr(self.model, 'clear_memory'):
            self.model.clear_memory()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_detector(model_name: str = "patchcore", 
                   resolution_ratio: float = 0.35,
                   **kwargs) -> GPUDeepAnomalyDetector:
    """
    ê²€ì¶œê¸° ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        resolution_ratio: í•´ìƒë„ ë¹„ìœ¨ (0.2-1.0)
        **kwargs: ì¶”ê°€ ëª¨ë¸ ì„¤ì •
    
    Returns:
        GPU ë”¥ëŸ¬ë‹ ê²€ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤
    """
    config = {'resolution_ratio': resolution_ratio}
    config.update(kwargs)
    
    detector = GPUDeepAnomalyDetector(model_name=model_name, config=config)
    return detector


def get_available_models() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return ModelFactory.get_available_models()


def get_model_info(model_name: str) -> Dict[str, Any]:
    """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    return ModelFactory.get_model_info(model_name)
