#!/usr/bin/env python3
"""
GPU ìƒíƒœ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
from pathlib import Path
import torch

# ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir / "src"))
sys.path.insert(0, str(current_dir / "models"))

def check_gpu_status():
    """GPU ìƒíƒœ ì „ì²´ ì ê²€"""
    print("=" * 60)
    print("ğŸ” GPU ìƒíƒœ ì¢…í•© ì ê²€")
    print("=" * 60)
    
    # PyTorch GPU ìƒíƒœ
    print(f"ğŸ PyTorch ë²„ì „: {torch.__version__}")
    print(f"ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"âš¡ CUDA ë²„ì „: {torch.version.cuda}")
        print(f"ğŸ–¥ï¸ GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"ğŸ“Š GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    else:
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    # ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
    print("\nğŸ“¦ ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸")
    try:
        from gpu_optimized_detector import create_detector
        print("âœ… gpu_optimized_detector import ì„±ê³µ")
    except Exception as e:
        print(f"âŒ gpu_optimized_detector import ì‹¤íŒ¨: {e}")
        return False
    
    # ê²€ì¶œê¸° ìƒì„± í…ŒìŠ¤íŠ¸
    print("\nğŸ¤– ê²€ì¶œê¸° ìƒì„± í…ŒìŠ¤íŠ¸")
    try:
        detector = create_detector()
        print("âœ… ê²€ì¶œê¸° ìƒì„± ì„±ê³µ")
        print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {detector.device}")
        print(f"ğŸ§  ëª¨ë¸: {detector.model_name}")
        
        # ëª¨ë¸ ì •ë³´
        info = detector.get_model_info()
        print(f"ğŸ“Š ê²€ì¶œê¸° ì •ë³´: {info}")
        
    except Exception as e:
        print(f"âŒ ê²€ì¶œê¸° ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ëª¨ë“  GPU ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
    print("=" * 60)
    return True

if __name__ == "__main__":
    check_gpu_status()
